import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os
import pickle
import torch # Added to check for CUDA
import warnings

"""
This is v3 of the meta-model training script.

It is designed to work with the '_v3.npy' files generated by
'create_stacking_features_v3.py'.

Those files contain:
- 15 prediction features (5 models * 3 classes)
- 10 original tabular features
- Total: 25 features

This script will train the meta-model on this richer 25-feature dataset.
"""

def train_meta_model():
    print("--- Training Level-2 Stacking Meta-Model (v3 - 25 Features) ---")

    # 1. Load the generated features
    print("Loading features from .npy files...")
    
    # --- THIS IS THE V3 CHANGE ---
    # We now load the '_v3' files, which have 25 features
    train_feat_file = 'oof_train_features_v3.npy'
    train_lbl_file = 'oof_train_labels_v3.npy'
    test_feat_file = 'oof_test_features_v3.npy'
    test_lbl_file = 'oof_test_labels_v3.npy'
    # --- END OF V3 CHANGE ---

    try:
        X_train = np.load(train_feat_file)
        y_train = np.load(train_lbl_file)
        X_test = np.load(test_feat_file)
        y_test = np.load(test_lbl_file)
    except FileNotFoundError as e:
        print(f"\nFATAL ERROR: File not found ({e}).")
        print("Please run 'create_stacking_features_v3.py' first.")
        return

    print(f"Training features shape: {X_train.shape}")
    print(f"Test features shape:     {X_test.shape}")
    
    if X_train.shape[1] != 25 or X_test.shape[1] != 25:
        print(f"WARNING: Expected 25 features, but found {X_train.shape[1]}.")
        print("This may be loading old v1/v2 files. Continuing anyway...")

    # 2. Scale the features
    # Scaling is CRITICAL here because the 15 prediction features (0-1)
    # and the 10 tabular features (e.g., Age 20-70) are on
    # completely different scales.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. Train the Meta-Model (Logistic Regression)
    print("\nTraining Logistic Regression meta-model...")
    
    warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn.linear_model')
    
    model = LogisticRegression(
        solver='liblinear',
        C=0.1,             
        multi_class='auto',
        random_state=42,
        max_iter=1000
    )
    
    model.fit(X_train_scaled, y_train)
    
    with open('stacking_meta_model_v3.pkl', 'wb') as f:
        pickle.dump(model, f)
    with open('stacking_scaler_v3.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("Saved 'stacking_meta_model_v3.pkl' and 'stacking_scaler_v3.pkl'")

    # 4. Evaluate on the Test Set
    print("\n--- FINAL STACKED MODEL RESULTS (Logistic Regression) ---")
    y_pred = model.predict(X_test_scaled)
    
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, 
        y_pred, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm = confusion_matrix(y_test, y_pred)

    print(f"Final Stacked Test Accuracy (Logistic Regression): {accuracy * 100:.4f}%")
    print("\nClassification Report:")
    print(report)
    print("\nConfusion Matrix:")
    print(cm)

    # --- 5. Train XGBoost meta-model ---
    # XGBoost is often the best model for this kind of stacked data.
    print("\nTraining XGBoost meta-model...")
    
    warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')

    xgb_model = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=3,
        n_estimators=1000, # More estimators for more features
        learning_rate=0.01, # Lower LR for better generalization
        max_depth=4,        # Slightly deeper tree
        subsample=0.7,
        colsample_bytree=0.7,
        use_label_encoder=False,
        eval_metric='mlogloss',
        early_stopping_rounds=50, # More patience
        tree_method='hist',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # We can use the scaled data, but XGB is robust to unscaled data too.
    # We will use the unscaled data here.
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train)], # Evaluate on OOF train data
        verbose=False
    )
    
    print("\n--- FINAL STACKED MODEL RESULTS (XGBoost) ---")
    y_pred_xgb = xgb_model.predict(X_test)
    
    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
    report_xgb = classification_report(
        y_test, 
        y_pred_xgb, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm_xgb = confusion_matrix(y_test, y_pred_xgb)

    print(f"Final Stacked Test Accuracy (XGBoost): {accuracy_xgb * 100:.4f}%")
    print("\nClassification Report (XGBoost):")
    print(report_xgb)
    print("\nConfusion Matrix (XGBoost):")
    print(cm_xgb)
    
    with open('stacking_meta_model_xgb_v3.pkl', 'wb') as f:
        pickle.dump(xgb_model, f)
    print("Saved 'stacking_meta_model_xgb_v3.pkl'")


if __name__ == "__main__":
    train_meta_model()