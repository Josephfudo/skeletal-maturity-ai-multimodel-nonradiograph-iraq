import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os
import pickle
import torch # Added to check for CUDA

"""
This script trains the "Level-2 Meta-Model" on the features generated by
'create_stacking_features.py'.

It loads the OOF (Out-of-Fold) predictions from the validation set and
uses them to train a simple classifier. This classifier learns the
optimal way to combine the predictions from all your 25 deep learning models.

It then evaluates this meta-model on the test set predictions.
"""

def train_meta_model():
    print("--- Training Level-2 Stacking Meta-Model ---")

    # 1. Load the generated features
    print("Loading features from .npy files...")
    try:
        X_train = np.load('oof_train_features.npy')
        y_train = np.load('oof_train_labels.npy')
        X_test = np.load('oof_test_features.npy')
        y_test = np.load('oof_test_labels.npy')
    except FileNotFoundError:
        print("\nFATAL ERROR: .npy files not found.")
        print("Please run 'create_stacking_features.py' first.")
        return

    print(f"Training features shape: {X_train.shape}")
    print(f"Test features shape:     {X_test.shape}")

    # 2. Scale the features
    # Predictions (probabilities) are already on a good scale (0-1), 
    # but scaling can still help Logistic Regression converge.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. Train the Meta-Model (Logistic Regression)
    # We use a simple, robust model to prevent overfitting the OOF predictions.
    # C=0.1 provides some regularization.
    print("\nTraining Logistic Regression meta-model...")
    model = LogisticRegression(
        solver='liblinear', # Good solver for smaller datasets
        C=0.1,
        multi_class='auto',
        random_state=42,
        max_iter=1000
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Save the trained meta-model and scaler
    with open('stacking_meta_model.pkl', 'wb') as f:
        pickle.dump(model, f)
    with open('stacking_scaler.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("Saved 'stacking_meta_model.pkl' and 'stacking_scaler.pkl'")

    # 4. Evaluate on the Test Set
    print("\n--- FINAL STACKED MODEL RESULTS (Logistic Regression) ---")
    y_pred = model.predict(X_test_scaled)
    
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, 
        y_pred, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm = confusion_matrix(y_test, y_pred)

    print(f"Final Stacked Test Accuracy (Logistic Regression): {accuracy * 100:.4f}%")
    print("\nClassification Report:")
    print(report)
    print("\nConfusion Matrix:")
    print(cm)

    # --- Optional: Try XGBoost as well ---
    # XGBoost is also a powerful meta-model
    print("\nTraining XGBoost meta-model...")
    xgb_model = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=3,
        n_estimators=200,
        learning_rate=0.05,
        max_depth=3,
        use_label_encoder=False,
        eval_metric='mlogloss',
        early_stopping_rounds=20,
        tree_method='hist',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # XGBoost doesn't strictly require scaled data
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train)], # Evaluate on training data for early stopping
        verbose=False
    )
    
    print("\n--- FINAL STACKED MODEL RESULTS (XGBoost) ---")
    y_pred_xgb = xgb_model.predict(X_test)
    
    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
    report_xgb = classification_report(
        y_test, 
        y_pred_xgb, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm_xgb = confusion_matrix(y_test, y_pred_xgb)

    print(f"Final Stacked Test Accuracy (XGBoost): {accuracy_xgb * 100:.4f}%")
    print("\nClassification Report (XGBoost):")
    print(report_xgb)
    print("\nConfusion Matrix (XGBoost):")
    print(cm_xgb)
    
    with open('stacking_meta_model_xgb.pkl', 'wb') as f:
        pickle.dump(xgb_model, f)
    print("Saved 'stacking_meta_model_xgb.pkl'")


if __name__ == "__main__":
    train_meta_model()