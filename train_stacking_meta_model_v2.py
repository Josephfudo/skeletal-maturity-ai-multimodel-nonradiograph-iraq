import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import os
import pickle
import torch # Added to check for CUDA
import warnings

"""
This script trains the "Level-2 Meta-Model" on the features generated by
'create_stacking_features_v2.py'.

It loads the OOF (Out-of-Fold) predictions from the validation set and
uses them to train a simple classifier. This classifier learns the
optimal way to combine the predictions from all your 25 deep learning models.

It then evaluates this meta-model on the test set predictions.
"""

def train_meta_model():
    print("--- Training Level-2 Stacking Meta-Model ---")

    # 1. Load the generated features
    print("Loading features from .npy files...")
    
    # --- THIS IS THE V2 CHANGE ---
    # We now load the '_v2' files, which have 15 features (5 models * 3 classes)
    train_feat_file = 'oof_train_features_v2.npy'
    train_lbl_file = 'oof_train_labels_v2.npy'
    test_feat_file = 'oof_test_features_v2.npy'
    test_lbl_file = 'oof_test_labels_v2.npy'
    # --- END OF V2 CHANGE ---

    try:
        X_train = np.load(train_feat_file)
        y_train = np.load(train_lbl_file)
        X_test = np.load(test_feat_file)
        y_test = np.load(test_lbl_file)
    except FileNotFoundError as e:
        print(f"\nFATAL ERROR: File not found ({e}).")
        print("Please run 'create_stacking_features_v2.py' first.")
        return

    print(f"Training features shape: {X_train.shape}")
    print(f"Test features shape:     {X_test.shape}")
    
    if X_train.shape[1] != 15 or X_test.shape[1] != 15:
        print(f"WARNING: Expected 15 features, but found {X_train.shape[1]}.")
        print("This may be loading old v1 files. Continuing anyway...")

    # 2. Scale the features
    # Predictions (probabilities) are already on a good scale (0-1), 
    # but scaling is crucial for Logistic Regression to converge well.
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # 3. Train the Meta-Model (Logistic Regression)
    print("\nTraining Logistic Regression meta-model...")
    
    # Suppress sklearn warnings about 'liblinear'
    warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn.linear_model')
    
    model = LogisticRegression(
        solver='liblinear', # Good solver for smaller datasets with L1/L2
        C=0.1,              # C=0.1 provides good regularization
        multi_class='auto', # Should automatically choose 'ovr'
        random_state=42,
        max_iter=1000
    )
    
    model.fit(X_train_scaled, y_train)
    
    # Save the trained meta-model and scaler
    with open('stacking_meta_model_v2.pkl', 'wb') as f:
        pickle.dump(model, f)
    with open('stacking_scaler_v2.pkl', 'wb') as f:
        pickle.dump(scaler, f)
    print("Saved 'stacking_meta_model_v2.pkl' and 'stacking_scaler_v2.pkl'")

    # 4. Evaluate on the Test Set
    print("\n--- FINAL STACKED MODEL RESULTS (Logistic Regression) ---")
    y_pred = model.predict(X_test_scaled)
    
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(
        y_test, 
        y_pred, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm = confusion_matrix(y_test, y_pred)

    print(f"Final Stacked Test Accuracy (Logistic Regression): {accuracy * 100:.4f}%")
    print("\nClassification Report:")
    print(report)
    print("\nConfusion Matrix:")
    print(cm)

    # --- Optional: Try XGBoost as well ---
    # XGBoost is also a powerful meta-model and doesn't need scaled data
    print("\nTraining XGBoost meta-model...")
    
    # Suppress XGBoost warnings
    warnings.filterwarnings('ignore', category=UserWarning, module='xgboost')

    xgb_model = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=3,
        n_estimators=500, # Increased estimators for more complex features
        learning_rate=0.02, # Slightly lower LR
        max_depth=3,
        subsample=0.8,
        colsample_bytree=0.8,
        use_label_encoder=False,
        eval_metric='mlogloss',
        early_stopping_rounds=50,
        tree_method='hist',
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    # Use the (unscaled) OOF training set to train, and also to find the best stopping point
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_train, y_train)], 
        verbose=False
    )
    
    print("\n--- FINAL STACKED MODEL RESULTS (XGBoost) ---")
    y_pred_xgb = xgb_model.predict(X_test)
    
    accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
    report_xgb = classification_report(
        y_test, 
        y_pred_xgb, 
        target_names=['Pre-peak', 'Peak', 'Post-peak'],
        digits=4
    )
    cm_xgb = confusion_matrix(y_test, y_pred_xgb)

    print(f"Final Stacked Test Accuracy (XGBoost): {accuracy_xgb * 100:.4f}%")
    print("\nClassification Report (XGBoost):")
    print(report_xgb)
    print("\nConfusion Matrix (XGBoost):")
    print(cm_xgb)
    
    with open('stacking_meta_model_xgb_v2.pkl', 'wb') as f:
        pickle.dump(xgb_model, f)
    print("Saved 'stacking_meta_model_xgb_v2.pkl'")


if __name__ == "__main__":
    train_meta_model()